{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1NSHAulKztIjJo6VSoHeVd5EKMfeEj-Tt","timestamp":1713962716399}],"authorship_tag":"ABX9TyOSpdCTqDx0EITBcul7+JL6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lBANFk7mHh7k","executionInfo":{"status":"ok","timestamp":1713962563674,"user_tz":-330,"elapsed":19,"user":{"displayName":"anushka Bhingade","userId":"11172444282330070396"}}},"outputs":[],"source":["!nvcc --version\n"]},{"cell_type":"code","source":["!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dUe0mVeIS5T","executionInfo":{"status":"ok","timestamp":1713962645870,"user_tz":-330,"elapsed":17346,"user":{"displayName":"anushka Bhingade","userId":"11172444282330070396"}},"outputId":"aab094b6-500b-47b8-998c-677c11456142"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n","  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-4qq6kgv2\n","  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-4qq6kgv2\n","  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 5741c522547756ac4bb7a16df32106a15efb8a57\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: nvcc4jupyter\n","  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10741 sha256=1c43b610d84440f376c57bb0b3d20f87e2433fdb6f06eb98d5fd7c81c461a971\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-l9ggpvb8/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n","Successfully built nvcc4jupyter\n","Installing collected packages: nvcc4jupyter\n","Successfully installed nvcc4jupyter-1.2.1\n"]}]},{"cell_type":"code","source":["%load_ext nvcc4jupyter"],"metadata":{"executionInfo":{"status":"ok","timestamp":1713962645871,"user_tz":-330,"elapsed":19,"user":{"displayName":"anushka Bhingade","userId":"11172444282330070396"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"aoBVpw2xIbxK","outputId":"18cd6c84-ec27-4fa0-fc24-1e8a6838b2d5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected platform \"Colab\". Running its setup...\n","Source files will be saved in \"/tmp/tmphmq9juku\".\n"]}]},{"cell_type":"code","source":["%%cuda\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <cuda_runtime.h>\n","\n","#define N 1000000 // Size of vectors\n","\n","// CUDA kernel for vector addition\n","__global__ void vectorAdd(int *a, int *b, int *c, int n) {\n","    int i = blockDim.x * blockIdx.x + threadIdx.x;\n","    if (i < n) {\n","        c[i] = a[i] + b[i];\n","    }\n","}\n","\n","int main() {\n","    // Host vectors and initialization\n","    int *h_a = (int *)malloc(N * sizeof(int)); // Allocate memory for host vector a\n","    int *h_b = (int *)malloc(N * sizeof(int)); // Allocate memory for host vector b\n","    int *h_c = (int *)malloc(N * sizeof(int)); // Allocate memory for host vector c\n","    // Initialize host vectors a and b\n","    for (int i = 0; i < N; ++i) {\n","        h_a[i] = i;\n","        h_b[i] = i * 2;\n","    }\n","\n","    // Device vectors\n","    int *d_a, *d_b, *d_c;\n","    cudaMalloc((void **)&d_a, N * sizeof(int)); // Allocate memory for device vector a\n","    cudaMalloc((void **)&d_b, N * sizeof(int)); // Allocate memory for device vector b\n","    cudaMalloc((void **)&d_c, N * sizeof(int)); // Allocate memory for device vector c\n","\n","    // Copy host vectors to device\n","    cudaMemcpy(d_a, h_a, N * sizeof(int), cudaMemcpyHostToDevice); // Copy host vector a to device\n","    cudaMemcpy(d_b, h_b, N * sizeof(int), cudaMemcpyHostToDevice); // Copy host vector b to device\n","\n","    // Launch vectorAdd kernel on GPU\n","    vectorAdd<<<ceil(N / 256.0), 256>>>(d_a, d_b, d_c, N); // Run kernel with appropriate grid and block dimensions\n","\n","    // Copy result from device to host\n","    cudaMemcpy(h_c, d_c, N * sizeof(int), cudaMemcpyDeviceToHost); // Copy device vector c to host\n","\n","    // Verify the result\n","    for (int i = 0; i < 10; ++i) { // Print the first 10 elements of vectors a, b, and c\n","        printf(\"%d + %d = %d\\n\", h_a[i], h_b[i], h_c[i]);\n","    }\n","\n","    // Free memory\n","    free(h_a); // Free memory allocated for host vector a\n","    free(h_b); // Free memory allocated for host vector b\n","    free(h_c); // Free memory allocated for host vector c\n","    cudaFree(d_a); // Free memory allocated for device vector a\n","    cudaFree(d_b); // Free memory allocated for device vector b\n","    cudaFree(d_c); // Free memory allocated for device vector c\n","\n","    return 0;\n","}\n","\n"],"metadata":{"executionInfo":{"status":"ok","timestamp":1713962677332,"user_tz":-330,"elapsed":4441,"user":{"displayName":"anushka Bhingade","userId":"11172444282330070396"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"nYJ8ACtDIcDA","outputId":"b437fc7a-e101-4201-9b2b-814e432b9fd5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["0 + 0 = 0\n","1 + 2 = 0\n","2 + 4 = 0\n","3 + 6 = 0\n","4 + 8 = 0\n","5 + 10 = 0\n","6 + 12 = 0\n","7 + 14 = 0\n","8 + 16 = 0\n","9 + 18 = 0\n","\n"]}]},{"cell_type":"code","source":["%%cuda\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <cuda_runtime.h>\n","\n","#define N 32 // Matrix size\n","\n","// CUDA kernel for matrix multiplication\n","__global__ void matrixMul(int *a, int *b, int *c, int n) {\n","    int row = blockIdx.y * blockDim.y + threadIdx.y;\n","    int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (row < n && col < n) {\n","        int sum = 0;\n","        // Compute dot product of row of matrix A and column of matrix B\n","        for (int i = 0; i < n; ++i) {\n","            sum += a[row * n + i] * b[i * n + col];\n","        }\n","        // Store result in the corresponding cell of matrix C\n","        c[row * n + col] = sum;\n","    }\n","}\n","\n","int main() {\n","    // Matrix dimensions\n","    int size = N * N * sizeof(int);\n","\n","    // Host matrices\n","    int *h_a, *h_b, *h_c;\n","\n","    // Device matrices\n","    int *d_a, *d_b, *d_c;\n","\n","    // Allocate memory for host matrices\n","    h_a = (int *)malloc(size);\n","    h_b = (int *)malloc(size);\n","    h_c = (int *)malloc(size);\n","\n","    // Initialize host matrices\n","    for (int i = 0; i < N * N; ++i) {\n","        h_a[i] = i;\n","        h_b[i] = i * 2;\n","    }\n","\n","    // Allocate memory for device matrices\n","    cudaMalloc((void **)&d_a, size);\n","    cudaMalloc((void **)&d_b, size);\n","    cudaMalloc((void **)&d_c, size);\n","\n","    // Copy host matrices to device\n","    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n","    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n","\n","    // Define grid and block size\n","    dim3 dimGrid(ceil(N / 16.0), ceil(N / 16.0), 1);\n","    dim3 dimBlock(16, 16, 1);\n","\n","    // Print input matrices\n","    printf(\"Matrix A (Input):\\n\");\n","    for (int i = 0; i < 4; ++i) {\n","        for (int j = 0; j < 4; ++j) {\n","            printf(\"%d \", h_a[i * N + j]);\n","        }\n","        printf(\"\\n\");\n","    }\n","\n","    printf(\"\\nMatrix B (Input):\\n\");\n","    for (int i = 0; i < 4; ++i) {\n","        for (int j = 0; j < 4; ++j) {\n","            printf(\"%d \", h_b[i * N + j]);\n","        }\n","        printf(\"\\n\");\n","    }\n","\n","    // Launch matrixMul kernel on GPU\n","    matrixMul<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, N);\n","\n","    // Copy result from device to host\n","    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n","\n","    // Print result matrix\n","    printf(\"\\nResult Matrix (Output):\\n\");\n","    for (int i = 0; i < 4; ++i) {\n","        for (int j = 0; j < 4; ++j) {\n","            printf(\"%d \", h_c[i * N + j]);\n","        }\n","        printf(\"\\n\");\n","    }\n","\n","    // Free device memory\n","    cudaFree(d_a);\n","    cudaFree(d_b);\n","    cudaFree(d_c);\n","\n","    // Free host memory\n","    free(h_a);\n","    free(h_b);\n","    free(h_c);\n","\n","    return 0;\n","}\n"],"metadata":{"executionInfo":{"status":"ok","timestamp":1713962701608,"user_tz":-330,"elapsed":1993,"user":{"displayName":"anushka Bhingade","userId":"11172444282330070396"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"Jr3LIUGuIcL4","outputId":"ea463edf-5cf0-4ec6-dbaa-f4552c0a3dd2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Matrix A (Input):\n","0 1 2 3 \n","32 33 34 35 \n","64 65 66 67 \n","96 97 98 99 \n","\n","Matrix B (Input):\n","0 2 4 6 \n","64 66 68 70 \n","128 130 132 134 \n","192 194 196 198 \n","\n","Result Matrix (Output):\n","0 0 0 0 \n","0 0 0 0 \n","0 0 0 0 \n","0 0 0 0 \n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"KTTxMoUxIcS-"},"execution_count":null,"outputs":[]}]}